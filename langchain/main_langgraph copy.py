######################################################
# Criando a base de uma solu√ß√£o com LCEL e LangGraph #
######################################################

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import os


# Carrega vari√°veis de ambiente do arquivo .env
load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')

# Inicializa o modelo de chat da OpenAI
# Usando GPT-4o-mini (vers√£o mais leve e econ√¥mica do GPT-4)
modelo = ChatOpenAI(
    model='gpt-4o-mini',  # Modelo GPT-4o-mini - mais r√°pido e barato que o GPT-4 completo
    temperature=0.5,  # Temperatura m√©dia - equil√≠brio entre criatividade e consist√™ncia
    api_key=api_key  # Chave da API
)

# Define o template de prompt com duas mensagens:
# 1. Mensagem do sistema: define o papel/comportamento do assistente
# 2. Mensagem humana: a pergunta do usu√°rio (vari√°vel {query})
prompt_consultor = ChatPromptTemplate.from_messages(
    [
        ('system', 'Voc√™ √© um consultor de viagens'),  # Contexto do sistema
        ('human', '{query}')  # Placeholder para a pergunta do usu√°rio
    ]
)

# Cria a cadeia de processamento (pipeline):
# 1. prompt_consultor: formata o prompt com a query do usu√°rio
# 2. modelo: processa o prompt e gera a resposta
# 3. StrOutputParser(): converte a resposta em string simples
assistente = prompt_consultor | modelo | StrOutputParser()

# Invoca a cadeia passando a query do usu√°rio
# A query substituir√° o placeholder {query} no prompt
response = assistente.invoke(
    {'query': 'Quero f√©rias em praias no Brasil.'}
)

# Exibe a resposta do modelo
print(response)

# üìù NOTA: Este c√≥digo N√ÉO tem mem√≥ria
# Cada invoke() √© uma intera√ß√£o independente, sem contexto de mensagens anteriores
# Para adicionar mem√≥ria, seria necess√°rio usar RunnableWithMessageHistory
